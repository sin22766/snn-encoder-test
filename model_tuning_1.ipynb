{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30cc7af7-5ca2-4964-9014-2001d1f6eca9",
   "metadata": {},
   "source": [
    "# Tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e6f1fa-ae91-44aa-9584-494fc333388c",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f717e36-0512-47ef-8ec6-c3d7e477d771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ee48cbb-2054-44cd-9474-0a4b7d54ed3f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['channels', 'data', 'info']>\n",
      "<KeysViewHDF5 ['channels', 'data', 'info']>\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./CHB-MIT/processed\"\n",
    "ictal_path = os.path.join(data_path, \"ictal.h5\")\n",
    "interictal_path = os.path.join(data_path, \"interictal.h5\")\n",
    "\n",
    "ictal_file = h5py.File(ictal_path, 'r')\n",
    "interictal_file = h5py.File(interictal_path, 'r')\n",
    "\n",
    "print(ictal_file.keys())\n",
    "print(interictal_file.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a6f5c2-184b-4f0a-8772-a6cd0fa52ed5",
   "metadata": {},
   "source": [
    "## Convert data to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62896f2e-72c7-4a80-8d31-2ebe862b8a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ictal data shape torch.Size([2509, 22, 2048])\n",
      "Interictal data shape torch.Size([2509, 22, 2048])\n"
     ]
    }
   ],
   "source": [
    "ictal_np = np.array(ictal_file['data'])\n",
    "interictal_np = np.array(interictal_file['data'])\n",
    "\n",
    "ictal_data = torch.tensor(ictal_np, dtype=torch.float32)\n",
    "interictal_data = torch.tensor(interictal_np, dtype=torch.float32)\n",
    "\n",
    "print(f\"Ictal data shape {ictal_data.shape}\")\n",
    "print(f\"Interictal data shape {interictal_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ed5a17-2f6e-48ca-8c19-f5648004befb",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe6d8219-cc64-44c4-b8e5-cf9a7252471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, ictal_data, interictal_data):\n",
    "        # Ensure the data is converted to tensors\n",
    "        self.data = torch.cat([ictal_data, interictal_data])\n",
    "        # Labels for ictal and interictal data\n",
    "        self.labels = torch.cat(\n",
    "            [\n",
    "                torch.ones(len(ictal_data)),  # Ictal = 1\n",
    "                torch.zeros(len(interictal_data)),  # Interictal = 0\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eeg_raw = self.data[idx]  # EEG data of shape (22, 2048)\n",
    "        label = self.labels[idx].bool()  # Label: 0 (interictal) or 1 (ictal)\n",
    "        return eeg_raw, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c77bd595",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EEGDataset(ictal_data, interictal_data)\n",
    "train_dataset, test_dataset, val_dataset = random_split(dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4dcacf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Train\n",
      "  Total samples: 3513\n",
      "  Ictal (seizure): 1770 (50.38%)\n",
      "  Interictal (normal): 1743 (49.62%)\n",
      "  Ratio: 1.02\n",
      "----------------------------------------\n",
      "Dataset: Test\n",
      "  Total samples: 1004\n",
      "  Ictal (seizure): 507 (50.50%)\n",
      "  Interictal (normal): 497 (49.50%)\n",
      "  Ratio: 1.02\n",
      "----------------------------------------\n",
      "Dataset: Validation\n",
      "  Total samples: 501\n",
      "  Ictal (seizure): 232 (46.31%)\n",
      "  Interictal (normal): 269 (53.69%)\n",
      "  Ratio: 0.86\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "subset_labels = (\"Train\", \"Test\", \"Validation\")\n",
    "\n",
    "for i, data in enumerate([train_dataset, test_dataset, val_dataset]):\n",
    "    label_counts = {0: 0, 1: 0}\n",
    "\n",
    "    for idx in data.indices:\n",
    "        label = data.dataset.labels[idx].item()\n",
    "        label_counts[label] += 1\n",
    "\n",
    "    total = sum(label_counts.values())\n",
    "    \n",
    "    print(f\"Dataset: {subset_labels[i]}\")\n",
    "    print(f\"  Total samples: {total}\")\n",
    "    print(f\"  Ictal (seizure): {label_counts[1]} ({label_counts[1]/total:.2%})\")\n",
    "    print(f\"  Interictal (normal): {label_counts[0]} ({label_counts[0]/total:.2%})\")\n",
    "    print(f\"  Ratio: {label_counts[1]/label_counts[0]:.2f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ecb71-7087-4c20-bd5d-e238c484ae76",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "From this sample model the data is not time domain but the frequency so it need to do the sfft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c370d8ed-1d90-4d9f-aeae-cfb63d865717",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate, SConv2dLSTM\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b9b2154-7bb5-4965-895f-9228bb20475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_stft(eeg_data, n_fft=256, hop_length=32, win_length=128):\n",
    "    \"\"\"\n",
    "    Apply STFT to batched EEG data using vectorization\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    eeg_data: torch.Tensor\n",
    "        EEG data with shape (batch, channels, time_steps)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    stft_output: torch.Tensor\n",
    "        STFT output with shape (batch, channels, frequency_bins, time_frames)\n",
    "    \"\"\"\n",
    "    batch_size, n_channels, time_steps = eeg_data.shape\n",
    "    window = torch.hann_window(win_length)\n",
    "\n",
    "    # Reshape to (batch*channels, time_steps)\n",
    "    reshaped_data = eeg_data.reshape(-1, time_steps)\n",
    "\n",
    "    # Apply STFT to all channels at once\n",
    "    stft = torch.stft(\n",
    "        reshaped_data,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        window=window,\n",
    "        return_complex=True,\n",
    "    )\n",
    "\n",
    "    # Reshape back to (batch, channels, freq_bins, time_frames)\n",
    "    freq_bins, time_frames = stft.shape[1], stft.shape[2]\n",
    "    stft_output = stft.reshape(batch_size, n_channels, freq_bins, time_frames)\n",
    "\n",
    "    return stft_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae383bfa-c4f5-465f-8493-30d21d47eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STFTSpikeClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels=22,\n",
    "        threshold=0.05,\n",
    "        slope=13.42287274232855,\n",
    "        beta=0.9181805491303656,\n",
    "        p1=0.5083664100388336,\n",
    "        p2=0.26260898840708335,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        spike_grad = surrogate.straight_through_estimator()\n",
    "        spike_grad2 = surrogate.fast_sigmoid(slope=slope)\n",
    "\n",
    "        # initialize layers - note input_channels=22 for your STFT data\n",
    "        self.lstm1 = SConv2dLSTM(\n",
    "            in_channels=input_channels,\n",
    "            out_channels=16,\n",
    "            kernel_size=3,\n",
    "            max_pool=(2, 1),\n",
    "            threshold=threshold,\n",
    "            spike_grad=spike_grad,\n",
    "        )\n",
    "        self.lstm2 = SConv2dLSTM(\n",
    "            in_channels=16,\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            max_pool=(2, 1),\n",
    "            threshold=threshold,\n",
    "            spike_grad=spike_grad,\n",
    "        )\n",
    "        self.lstm3 = snn.SConv2dLSTM(\n",
    "            in_channels=32,\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            max_pool=(2, 1),\n",
    "            threshold=threshold,\n",
    "            spike_grad=spike_grad,\n",
    "        )\n",
    "\n",
    "        # Calculate the flattened size based on your frequency dimension (129)\n",
    "        # After 3 max-pooling layers (each dividing by 2), size becomes: 129 → 64 → 32 → 16\n",
    "        # For time dimension: 1 (we process one time step at a time)\n",
    "        self.fc1 = nn.Linear(\n",
    "            64 * 16 * 1, 512\n",
    "        )  # Adjust this based on actual output size\n",
    "\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad2, threshold=threshold)\n",
    "        self.dropout1 = nn.Dropout(p1)\n",
    "        self.fc2 = nn.Linear(512, 2)  # Assuming binary classification\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad2, threshold=threshold)\n",
    "        self.dropout2 = nn.Dropout(p2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, channels=22, freq=129, time=57)\n",
    "        time_steps = x.size(3)\n",
    "\n",
    "        # Initialize LIF state variables\n",
    "        mem4 = self.lif1.init_leaky()\n",
    "        mem5 = self.lif2.init_leaky()\n",
    "        syn1, mem1 = self.lstm1.init_sconv2dlstm()\n",
    "        syn2, mem2 = self.lstm2.init_sconv2dlstm()\n",
    "        syn3, mem3 = self.lstm3.init_sconv2dlstm()\n",
    "\n",
    "        # Output recording\n",
    "        spk5_rec = []\n",
    "        mem5_rec = []\n",
    "\n",
    "        # Process each time step\n",
    "        for step in range(time_steps):\n",
    "            # Extract the current time step and prepare input\n",
    "            # x_t shape: (batch, channels=22, freq=129, time=1)\n",
    "            x_t = x[:, :, :, step].unsqueeze(-1)\n",
    "\n",
    "            # Pass through SConv2dLSTM layers\n",
    "            spk1, syn1, mem1 = self.lstm1(x_t, syn1, mem1)\n",
    "            spk2, syn2, mem2 = self.lstm2(spk1, syn2, mem2)\n",
    "            spk3, syn3, mem3 = self.lstm3(spk2, syn3, mem3)\n",
    "\n",
    "            # Flatten and feed through fully connected layers\n",
    "            cur4 = self.dropout1(self.fc1(spk3.flatten(1)))\n",
    "            spk4, mem4 = self.lif1(cur4, mem4)\n",
    "\n",
    "            cur5 = self.dropout2(self.fc2(spk4))\n",
    "            spk5, mem5 = self.lif2(cur5, mem5)\n",
    "\n",
    "            # Record output spikes and membrane potentials\n",
    "            spk5_rec.append(spk5)\n",
    "            mem5_rec.append(mem5)\n",
    "\n",
    "        # Stack time steps\n",
    "        return torch.stack(spk5_rec), torch.stack(mem5_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93c99fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch.functional as SF\n",
    "from snntorch import spikegen\n",
    "import optuna\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b10d6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    # Model hyperparameters (as before)\n",
    "    threshold = trial.suggest_float(\"threshold\", 0.01, 0.1)\n",
    "    slope = trial.suggest_float(\"slope\", 5.0, 20.0)\n",
    "    beta = trial.suggest_float(\"beta\", 0.8, 0.99)\n",
    "    p1 = trial.suggest_float(\"p1\", 0.3, 0.7)\n",
    "    p2 = trial.suggest_float(\"p2\", 0.1, 0.4)\n",
    "\n",
    "    # Optimizer hyperparameters\n",
    "    lr = trial.suggest_float(\"lr\", 1e-6, 1e-4, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-4, log=True)\n",
    "\n",
    "    # Scheduler hyperparameters\n",
    "    scheduler_factor = trial.suggest_float(\"scheduler_factor\", 0.1, 0.7)\n",
    "    scheduler_patience = trial.suggest_int(\"scheduler_patience\", 3, 10)\n",
    "\n",
    "    # Create model and optimizer\n",
    "    model = STFTSpikeClassifier(\n",
    "        input_channels=22, threshold=threshold, slope=slope, beta=beta, p1=p1, p2=p2\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Create scheduler with sampled parameters\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"min\",\n",
    "        factor=scheduler_factor,\n",
    "        patience=scheduler_patience,\n",
    "        min_lr=1e-6,\n",
    "    )\n",
    "\n",
    "    criterion = SF.mse_count_loss()\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 15  # Reduced for hyperparameter search\n",
    "    best_val_loss = 0\n",
    "\n",
    "    print(f\"Trial {trial.number} Starting training...\")\n",
    "    print(f\"Model Parameters: threshold={threshold}, slope={slope}, beta={beta}, p1={p1}, p2={p2}\")\n",
    "    print(f\"Optimizer Parameters: lr={lr}, weight_decay={weight_decay}\")\n",
    "    print(f\"Scheduler Parameters: factor={scheduler_factor}, patience={scheduler_patience}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        \n",
    "        for batch_idx, (data, targets) in enumerate(train_loop):\n",
    "            # Preprocess data\n",
    "            scaled_data = vectorized_stft(data)\n",
    "\n",
    "            encoding_method = trial.suggest_categorical(\n",
    "                \"encoding_method\", [\"rate\", \"delta\"]\n",
    "            )\n",
    "\n",
    "            if encoding_method == \"rate\":\n",
    "                scaled_data = torch.abs(scaled_data)\n",
    "\n",
    "                if scaled_data.max() > 0:  # Avoid division by zero\n",
    "                    scaled_data = scaled_data / scaled_data.max()\n",
    "\n",
    "                data_spike = spikegen.rate(scaled_data, time_var_input=True)\n",
    "            elif encoding_method == \"delta\":\n",
    "                # Get magnitude with sign from real part\n",
    "                magnitude = torch.abs(scaled_data)\n",
    "                sign = torch.sign(scaled_data.real)\n",
    "                \n",
    "                # Apply sign to magnitude to preserve direction\n",
    "                signed_magnitude = magnitude * sign\n",
    "\n",
    "                signed_magnitude = signed_magnitude / torch.max(magnitude)\n",
    "\n",
    "                data_spike = spikegen.delta(signed_magnitude)\n",
    "\n",
    "            data_spike, targets = data_spike.to(device), targets.to(device)\n",
    "\n",
    "            spk_rec, _ = model(data_spike)\n",
    "\n",
    "            loss_val = criterion(spk_rec, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss_val.item()\n",
    "            spike_sum = torch.sum(spk_rec, dim=0)\n",
    "            _, predicted = torch.max(spike_sum, 1)\n",
    "            total_train += targets.size(0)\n",
    "            correct_train += (predicted == targets).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            train_loop.set_postfix(\n",
    "                loss=train_loss / (batch_idx + 1), acc=100.0 * correct_train / total_train\n",
    "            )\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "\n",
    "            for batch_idx, (data, targets) in enumerate(val_loop):\n",
    "                # Preprocess data\n",
    "                scaled_data = vectorized_stft(data)\n",
    "\n",
    "                encoding_method = trial.suggest_categorical(\n",
    "                    \"encoding_method\", [\"rate\", \"delta\"]\n",
    "                )\n",
    "\n",
    "                if encoding_method == \"rate\":\n",
    "                    scaled_data = torch.abs(scaled_data)\n",
    "\n",
    "                    if scaled_data.max() > 0:  # Avoid division by zero\n",
    "                        scaled_data = scaled_data / scaled_data.max()\n",
    "\n",
    "                    data_spike = spikegen.rate(scaled_data, time_var_input=True)\n",
    "                elif encoding_method == \"delta\":\n",
    "                    # Get magnitude with sign from real part\n",
    "                    magnitude = torch.abs(scaled_data)\n",
    "                    sign = torch.sign(scaled_data.real)\n",
    "                    \n",
    "                    # Apply sign to magnitude to preserve direction\n",
    "                    signed_magnitude = magnitude * sign\n",
    "\n",
    "                    signed_magnitude = signed_magnitude / torch.max(magnitude)\n",
    "\n",
    "                    data_spike = spikegen.delta(signed_magnitude)\n",
    "\n",
    "\n",
    "                data_spike, targets = data_spike.to(device), targets.to(device)\n",
    "\n",
    "                spk_rec, _ = model(data_spike)\n",
    "\n",
    "                loss_val = criterion(spk_rec, targets)\n",
    "\n",
    "                val_loss += loss_val.item()\n",
    "                spike_sum = torch.sum(spk_rec, dim=0)\n",
    "                _, predicted = torch.max(spike_sum, 1)\n",
    "                total_val += targets.size(0)\n",
    "                correct_val += (predicted == targets).sum().item()\n",
    "\n",
    "                # Update progress bar\n",
    "                val_loop.set_postfix(\n",
    "                    loss=val_loss / (batch_idx + 1), acc=100.0 * correct_val / total_val\n",
    "                )\n",
    "\n",
    "        # Calculate average metrics\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        trial.report(avg_val_loss, epoch)\n",
    "\n",
    "        if avg_val_loss > best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "        \n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eaa5dbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-29 13:09:23,784] A new study created in RDB with name: STFT_SNN_Classifier\n"
     ]
    }
   ],
   "source": [
    "study_name = \"STFT_SNN_Classifier\"\n",
    "study_storage = \"sqlite:///classifier_tuning.db\"\n",
    "study = optuna.create_study(direction='minimize', study_name=study_name, storage=study_storage, load_if_exists=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223435a7",
   "metadata": {},
   "source": [
    "Try optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c2cd25a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Starting training...\n",
      "Model Parameters: threshold=0.029502183721464022, slope=5.877186513794134, beta=0.8878529652241832, p1=0.5158072293841036, p2=0.33379760536677766\n",
      "Optimizer Parameters: lr=4.0554645367364053e-05, weight_decay=1.45773874831512e-06\n",
      "Scheduler Parameters: factor=0.5512718333066357, patience=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [Train]: 100%|██████████| 110/110 [00:59<00:00,  1.84it/s, acc=71, loss=13.9]  \n",
      "Epoch 1/15 [Val]: 100%|██████████| 16/16 [00:05<00:00,  2.91it/s, acc=75.4, loss=11.5]\n",
      "Epoch 2/15 [Train]: 100%|██████████| 110/110 [01:00<00:00,  1.81it/s, acc=78.3, loss=10.6]\n",
      "Epoch 2/15 [Val]: 100%|██████████| 16/16 [00:05<00:00,  2.88it/s, acc=77.4, loss=10.3]\n",
      "Epoch 3/15 [Train]: 100%|██████████| 110/110 [01:01<00:00,  1.79it/s, acc=82.1, loss=9.14]\n",
      "Epoch 3/15 [Val]: 100%|██████████| 16/16 [00:05<00:00,  2.76it/s, acc=81.8, loss=8.77]\n",
      "Epoch 4/15 [Train]: 100%|██████████| 110/110 [01:01<00:00,  1.78it/s, acc=81.7, loss=9.05]\n",
      "Epoch 4/15 [Val]: 100%|██████████| 16/16 [00:05<00:00,  2.86it/s, acc=82, loss=8.97]  \n",
      "Epoch 5/15 [Train]: 100%|██████████| 110/110 [01:01<00:00,  1.80it/s, acc=82.1, loss=8.7] \n",
      "Epoch 5/15 [Val]: 100%|██████████| 16/16 [00:05<00:00,  2.89it/s, acc=83, loss=8.07]  \n",
      "Epoch 6/15 [Train]: 100%|██████████| 110/110 [01:01<00:00,  1.78it/s, acc=83.7, loss=8.33]\n",
      "Epoch 6/15 [Val]: 100%|██████████| 16/16 [00:05<00:00,  2.85it/s, acc=82.8, loss=8.54]\n",
      "Epoch 7/15 [Train]: 100%|██████████| 110/110 [01:01<00:00,  1.79it/s, acc=82.3, loss=8.53]\n",
      "Epoch 7/15 [Val]: 100%|██████████| 16/16 [00:06<00:00,  2.63it/s, acc=85.4, loss=7.74]\n",
      "Epoch 8/15 [Train]: 100%|██████████| 110/110 [01:01<00:00,  1.80it/s, acc=83.2, loss=8.38]\n",
      "Epoch 8/15 [Val]: 100%|██████████| 16/16 [00:05<00:00,  2.87it/s, acc=81.8, loss=8.7] \n",
      "Epoch 9/15 [Train]: 100%|██████████| 110/110 [01:00<00:00,  1.81it/s, acc=83.4, loss=8.16]\n",
      "Epoch 9/15 [Val]: 100%|██████████| 16/16 [00:05<00:00,  2.87it/s, acc=84.8, loss=7.71]\n",
      "Epoch 10/15 [Train]: 100%|██████████| 110/110 [01:01<00:00,  1.80it/s, acc=83.8, loss=8.02]\n",
      "Epoch 10/15 [Val]: 100%|██████████| 16/16 [00:05<00:00,  2.80it/s, acc=83.8, loss=8.56]\n",
      "Epoch 11/15 [Train]: 100%|██████████| 110/110 [01:01<00:00,  1.80it/s, acc=84.1, loss=7.93]\n",
      "Epoch 11/15 [Val]: 100%|██████████| 16/16 [00:05<00:00,  2.89it/s, acc=84, loss=7.97]  \n",
      "Epoch 12/15 [Train]: 100%|██████████| 110/110 [01:00<00:00,  1.80it/s, acc=83.9, loss=8.07]\n",
      "Epoch 12/15 [Val]: 100%|██████████| 16/16 [00:05<00:00,  2.88it/s, acc=81, loss=8.63]  \n",
      "Epoch 13/15 [Train]: 100%|██████████| 110/110 [01:00<00:00,  1.81it/s, acc=83.8, loss=7.9] \n",
      "Epoch 13/15 [Val]: 100%|██████████| 16/16 [00:05<00:00,  2.82it/s, acc=85.4, loss=7.57]\n",
      "Epoch 14/15 [Train]: 100%|██████████| 110/110 [01:00<00:00,  1.83it/s, acc=85.7, loss=7.56]\n",
      "Epoch 14/15 [Val]: 100%|██████████| 16/16 [00:05<00:00,  2.92it/s, acc=81, loss=9.31]  \n",
      "Epoch 15/15 [Train]: 100%|██████████| 110/110 [01:00<00:00,  1.82it/s, acc=84.2, loss=7.78]\n",
      "Epoch 15/15 [Val]: 100%|██████████| 16/16 [00:05<00:00,  2.85it/s, acc=81.2, loss=8.59]\n",
      "[I 2025-03-29 15:42:13,233] Trial 5 finished with value: 8.591924965381622 and parameters: {'threshold': 0.029502183721464022, 'slope': 5.877186513794134, 'beta': 0.8878529652241832, 'p1': 0.5158072293841036, 'p2': 0.33379760536677766, 'lr': 4.0554645367364053e-05, 'weight_decay': 1.45773874831512e-06, 'scheduler_factor': 0.5512718333066357, 'scheduler_patience': 6, 'encoding_method': 'rate'}. Best is trial 5 with value: 8.591924965381622.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 Starting training...\n",
      "Model Parameters: threshold=0.054468577834123635, slope=15.223145953482412, beta=0.8469335181205, p1=0.6309073641251619, p2=0.32738101660601415\n",
      "Optimizer Parameters: lr=1.3707996208326852e-06, weight_decay=1.1648432256981654e-05\n",
      "Scheduler Parameters: factor=0.36429153668934655, patience=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [Train]: 100%|██████████| 110/110 [00:59<00:00,  1.84it/s, acc=49.6, loss=27]  \n",
      "Epoch 1/15 [Val]: 100%|██████████| 16/16 [00:05<00:00,  2.89it/s, acc=53.7, loss=30]  \n",
      "[I 2025-03-29 15:43:18,593] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 Starting training...\n",
      "Model Parameters: threshold=0.03079676758506992, slope=5.530382745445492, beta=0.959057173449269, p1=0.4331110788193915, p2=0.32349814845088487\n",
      "Optimizer Parameters: lr=3.595027939747839e-06, weight_decay=1.5417379134228117e-05\n",
      "Scheduler Parameters: factor=0.37218934790231617, patience=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [Train]: 100%|██████████| 110/110 [01:00<00:00,  1.83it/s, acc=68.9, loss=15.9]\n",
      "Epoch 1/15 [Val]: 100%|██████████| 16/16 [00:05<00:00,  2.87it/s, acc=75.6, loss=14.1]\n",
      "[I 2025-03-29 15:44:24,522] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 Starting training...\n",
      "Model Parameters: threshold=0.051357653057170015, slope=16.80596182659699, beta=0.8115718799492017, p1=0.5281200883681392, p2=0.3903708933624829\n",
      "Optimizer Parameters: lr=3.39145932461514e-05, weight_decay=2.1952703721429205e-05\n",
      "Scheduler Parameters: factor=0.3133760298244207, patience=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [Train]: 100%|██████████| 110/110 [00:56<00:00,  1.95it/s, acc=61.2, loss=16.1]\n",
      "Epoch 1/15 [Val]: 100%|██████████| 16/16 [00:05<00:00,  3.14it/s, acc=63.9, loss=16.7]\n",
      "[I 2025-03-29 15:45:26,146] Trial 8 pruned. \n"
     ]
    }
   ],
   "source": [
    "study.optimize(objective, n_trials=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48878242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.029502183721464022,\n",
       " 'slope': 5.877186513794134,\n",
       " 'beta': 0.8878529652241832,\n",
       " 'p1': 0.5158072293841036,\n",
       " 'p2': 0.33379760536677766,\n",
       " 'lr': 4.0554645367364053e-05,\n",
       " 'weight_decay': 1.45773874831512e-06,\n",
       " 'scheduler_factor': 0.5512718333066357,\n",
       " 'scheduler_patience': 6,\n",
       " 'encoding_method': 'rate'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
