{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30cc7af7-5ca2-4964-9014-2001d1f6eca9",
   "metadata": {},
   "source": [
    "# Tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e6f1fa-ae91-44aa-9584-494fc333388c",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f717e36-0512-47ef-8ec6-c3d7e477d771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ed5a17-2f6e-48ca-8c19-f5648004befb",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6d8219-cc64-44c4-b8e5-cf9a7252471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for EEG data with seizure/non-seizure labels.\n",
    "    Can load data directly from HDF5 files.\n",
    "\n",
    "    Attributes:\n",
    "        data (torch.Tensor): Combined EEG data\n",
    "        labels (torch.Tensor): Binary labels (1 for ictal, 0 for interictal)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, data_dir, ictal_filename=\"ictal.h5\", interictal_filename=\"interictal.h5\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with ictal and interictal data,\n",
    "        either directly provided or loaded from files.\n",
    "\n",
    "        Parameters:\n",
    "            data_dir (str): Directory containing HDF5 data files\n",
    "            ictal_filename (str, optional): Filename for ictal data\n",
    "            interictal_filename (str, optional): Filename for interictal data\n",
    "        \"\"\"\n",
    "\n",
    "        ictal_path = os.path.join(data_dir, ictal_filename)\n",
    "        interictal_path = os.path.join(data_dir, interictal_filename)\n",
    "\n",
    "        ictal_file = h5py.File(ictal_path, \"r\")\n",
    "        interictal_file = h5py.File(interictal_path, \"r\")\n",
    "\n",
    "        ictal_data = torch.tensor(np.array(ictal_file[\"data\"]), dtype=torch.float32)\n",
    "        interictal_data = torch.tensor(\n",
    "            np.array(interictal_file[\"data\"]), dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        # Ensure the data is converted to tensors\n",
    "        self.data = torch.cat([ictal_data, interictal_data])\n",
    "        # Labels for ictal and interictal data\n",
    "        self.labels = torch.cat(\n",
    "            [\n",
    "                torch.ones(len(ictal_data)),  # Ictal = 1\n",
    "                torch.zeros(len(interictal_data)),  # Interictal = 0\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eeg_raw = self.data[idx]  # EEG data of shape (22, 2048)\n",
    "        label = self.labels[idx].bool()  # Label: 0 (interictal) or 1 (ictal)\n",
    "        return eeg_raw, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c77bd595",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./CHB-MIT/processed\"\n",
    "dataset = EEGDataset(data_path)\n",
    "train_dataset, test_dataset, val_dataset = random_split(dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ecb71-7087-4c20-bd5d-e238c484ae76",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "From this sample model the data is not time domain but the frequency so it need to do the sfft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c370d8ed-1d90-4d9f-aeae-cfb63d865717",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate, SConv2dLSTM\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b9b2154-7bb5-4965-895f-9228bb20475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_stft(eeg_data, n_fft=256, hop_length=32, win_length=128):\n",
    "    \"\"\"\n",
    "    Apply STFT to batched EEG data using vectorization\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    eeg_data: torch.Tensor\n",
    "        EEG data with shape (batch, channels, time_steps)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    stft_output: torch.Tensor\n",
    "        STFT output with shape (batch, channels, frequency_bins, time_frames)\n",
    "    \"\"\"\n",
    "    batch_size, n_channels, time_steps = eeg_data.shape\n",
    "    window = torch.hann_window(win_length)\n",
    "\n",
    "    # Reshape to (batch*channels, time_steps)\n",
    "    reshaped_data = eeg_data.reshape(-1, time_steps)\n",
    "\n",
    "    # Apply STFT to all channels at once\n",
    "    stft = torch.stft(\n",
    "        reshaped_data,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        window=window,\n",
    "        return_complex=True,\n",
    "    )\n",
    "\n",
    "    # Reshape back to (batch, channels, freq_bins, time_frames)\n",
    "    freq_bins, time_frames = stft.shape[1], stft.shape[2]\n",
    "    stft_output = stft.reshape(batch_size, n_channels, freq_bins, time_frames)\n",
    "\n",
    "    return stft_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae383bfa-c4f5-465f-8493-30d21d47eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STFTSpikeClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels=22,\n",
    "        threshold=0.05,\n",
    "        slope=13.42287274232855,\n",
    "        beta=0.9181805491303656,\n",
    "        p1=0.5083664100388336,\n",
    "        p2=0.26260898840708335,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        spike_grad = surrogate.straight_through_estimator()\n",
    "        spike_grad2 = surrogate.fast_sigmoid(slope=slope)\n",
    "\n",
    "        # initialize layers - note input_channels=22 for your STFT data\n",
    "        self.lstm1 = SConv2dLSTM(\n",
    "            in_channels=input_channels,\n",
    "            out_channels=16,\n",
    "            kernel_size=3,\n",
    "            max_pool=(2, 1),\n",
    "            threshold=threshold,\n",
    "            spike_grad=spike_grad,\n",
    "        )\n",
    "        self.lstm2 = SConv2dLSTM(\n",
    "            in_channels=16,\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            max_pool=(2, 1),\n",
    "            threshold=threshold,\n",
    "            spike_grad=spike_grad,\n",
    "        )\n",
    "        self.lstm3 = snn.SConv2dLSTM(\n",
    "            in_channels=32,\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            max_pool=(2, 1),\n",
    "            threshold=threshold,\n",
    "            spike_grad=spike_grad,\n",
    "        )\n",
    "\n",
    "        # Calculate the flattened size based on your frequency dimension (129)\n",
    "        # After 3 max-pooling layers (each dividing by 2), size becomes: 129 → 64 → 32 → 16\n",
    "        # For time dimension: 1 (we process one time step at a time)\n",
    "        self.fc1 = nn.Linear(\n",
    "            64 * 16 * 1, 512\n",
    "        )  # Adjust this based on actual output size\n",
    "\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad2, threshold=threshold)\n",
    "        self.dropout1 = nn.Dropout(p1)\n",
    "        self.fc2 = nn.Linear(512, 2)  # Assuming binary classification\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad2, threshold=threshold)\n",
    "        self.dropout2 = nn.Dropout(p2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, channels=22, freq=129, time=57)\n",
    "        time_steps = x.size(3)\n",
    "\n",
    "        # Initialize LIF state variables\n",
    "        mem4 = self.lif1.init_leaky()\n",
    "        mem5 = self.lif2.init_leaky()\n",
    "        syn1, mem1 = self.lstm1.init_sconv2dlstm()\n",
    "        syn2, mem2 = self.lstm2.init_sconv2dlstm()\n",
    "        syn3, mem3 = self.lstm3.init_sconv2dlstm()\n",
    "\n",
    "        # Output recording\n",
    "        spk5_rec = []\n",
    "        mem5_rec = []\n",
    "\n",
    "        # Process each time step\n",
    "        for step in range(time_steps):\n",
    "            # Extract the current time step and prepare input\n",
    "            # x_t shape: (batch, channels=22, freq=129, time=1)\n",
    "            x_t = x[:, :, :, step].unsqueeze(-1)\n",
    "\n",
    "            # Pass through SConv2dLSTM layers\n",
    "            spk1, syn1, mem1 = self.lstm1(x_t, syn1, mem1)\n",
    "            spk2, syn2, mem2 = self.lstm2(spk1, syn2, mem2)\n",
    "            spk3, syn3, mem3 = self.lstm3(spk2, syn3, mem3)\n",
    "\n",
    "            # Flatten and feed through fully connected layers\n",
    "            cur4 = self.dropout1(self.fc1(spk3.flatten(1)))\n",
    "            spk4, mem4 = self.lif1(cur4, mem4)\n",
    "\n",
    "            cur5 = self.dropout2(self.fc2(spk4))\n",
    "            spk5, mem5 = self.lif2(cur5, mem5)\n",
    "\n",
    "            # Record output spikes and membrane potentials\n",
    "            spk5_rec.append(spk5)\n",
    "            mem5_rec.append(mem5)\n",
    "\n",
    "        # Stack time steps\n",
    "        return torch.stack(spk5_rec), torch.stack(mem5_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93c99fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch.functional as SF\n",
    "from snntorch import spikegen\n",
    "import optuna\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10d6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define all hyperparameters in a single dictionary\n",
    "    params = {\n",
    "        # Model hyperparameters\n",
    "        \"threshold\": trial.suggest_float(\"threshold\", 0.01, 0.1),\n",
    "        \"slope\": trial.suggest_float(\"slope\", 5.0, 20.0),\n",
    "        \"beta\": trial.suggest_float(\"beta\", 0.8, 0.99),\n",
    "        \"p1\": trial.suggest_float(\"p1\", 0.3, 0.7),\n",
    "        \"p2\": trial.suggest_float(\"p2\", 0.1, 0.4),\n",
    "        # Optimizer hyperparameters\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-6, 1e-4, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-4, log=True),\n",
    "        # Scheduler hyperparameters\n",
    "        \"scheduler_factor\": trial.suggest_float(\"scheduler_factor\", 0.1, 0.7),\n",
    "        \"scheduler_patience\": trial.suggest_int(\"scheduler_patience\", 3, 10),\n",
    "    }\n",
    "\n",
    "    # Create model and optimizer using parameters from the dictionary\n",
    "    model = STFTSpikeClassifier(\n",
    "        input_channels=22,\n",
    "        threshold=params[\"threshold\"],\n",
    "        slope=params[\"slope\"],\n",
    "        beta=params[\"beta\"],\n",
    "        p1=params[\"p1\"],\n",
    "        p2=params[\"p2\"],\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=params[\"lr\"],\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=params[\"weight_decay\"],\n",
    "    )\n",
    "\n",
    "    # Create scheduler with parameters from the dictionary\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"min\",\n",
    "        factor=params[\"scheduler_factor\"],\n",
    "        patience=params[\"scheduler_patience\"],\n",
    "        min_lr=1e-6,\n",
    "    )\n",
    "\n",
    "    criterion = SF.mse_count_loss()\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 15  # Reduced for hyperparameter search\n",
    "    best_val_loss = 0\n",
    "\n",
    "    print(f\"Trial {trial.number} Starting training...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "\n",
    "        for batch_idx, (data, targets) in enumerate(train_loop):\n",
    "            # Preprocess data\n",
    "            scaled_data = vectorized_stft(data)\n",
    "\n",
    "            scaled_data = torch.abs(scaled_data)\n",
    "\n",
    "            if scaled_data.max() > 0:  # Avoid division by zero\n",
    "                scaled_data = scaled_data / scaled_data.max()\n",
    "\n",
    "            data_spike = spikegen.rate(scaled_data, time_var_input=True)\n",
    "\n",
    "            data_spike, targets = data_spike.to(device), targets.to(device)\n",
    "\n",
    "            spk_rec, _ = model(data_spike)\n",
    "\n",
    "            loss_val = criterion(spk_rec, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss_val.item()\n",
    "            spike_sum = torch.sum(spk_rec, dim=0)\n",
    "            _, predicted = torch.max(spike_sum, 1)\n",
    "            total_train += targets.size(0)\n",
    "            correct_train += (predicted == targets).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            train_loop.set_postfix(\n",
    "                loss=train_loss / (batch_idx + 1),\n",
    "                acc=100.0 * correct_train / total_train,\n",
    "            )\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\", leave=False)\n",
    "\n",
    "            for batch_idx, (data, targets) in enumerate(val_loop):\n",
    "                # Preprocess data\n",
    "                scaled_data = vectorized_stft(data)\n",
    "\n",
    "                scaled_data = torch.abs(scaled_data)\n",
    "\n",
    "                if scaled_data.max() > 0:  # Avoid division by zero\n",
    "                    scaled_data = scaled_data / scaled_data.max()\n",
    "\n",
    "                data_spike = spikegen.rate(scaled_data, time_var_input=True)\n",
    "\n",
    "                data_spike, targets = data_spike.to(device), targets.to(device)\n",
    "\n",
    "                spk_rec, _ = model(data_spike)\n",
    "\n",
    "                loss_val = criterion(spk_rec, targets)\n",
    "\n",
    "                val_loss += loss_val.item()\n",
    "                spike_sum = torch.sum(spk_rec, dim=0)\n",
    "                _, predicted = torch.max(spike_sum, 1)\n",
    "                total_val += targets.size(0)\n",
    "                correct_val += (predicted == targets).sum().item()\n",
    "\n",
    "                # Update progress bar\n",
    "                val_loop.set_postfix(\n",
    "                    loss=val_loss / (batch_idx + 1), acc=100.0 * correct_val / total_val\n",
    "                )\n",
    "\n",
    "        # Calculate average metrics\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        trial.report(avg_val_loss, epoch)\n",
    "\n",
    "        if avg_val_loss > best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaa5dbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-02 01:44:44,575] A new study created in RDB with name: Classifier Rate Encoder Old\n"
     ]
    }
   ],
   "source": [
    "from config import DB_CONFIG\n",
    "\n",
    "study_name = \"Classifier Rate Encoder Old\"\n",
    "storage_url = f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    study_name=study_name,\n",
    "    storage=storage_url,\n",
    "    load_if_exists=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223435a7",
   "metadata": {},
   "source": [
    "Try optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48878242",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
