{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30cc7af7-5ca2-4964-9014-2001d1f6eca9",
   "metadata": {},
   "source": [
    "# Tuning the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1de033f",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "Load the preprocessed data which prepared in HDF5 format. The data is split into training, validation, and test sets. The training set is used to train the model, the validation set is used to tune the hyperparameters, and the test set is used to evaluate the final model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c77bd595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CHBMITPreprocessedDataset\n",
    "\n",
    "dataset = CHBMITPreprocessedDataset('./CHB-MIT/processed_data.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ecb71-7087-4c20-bd5d-e238c484ae76",
   "metadata": {},
   "source": [
    "## Assemble the model\n",
    "\n",
    "We will use the Pytorch Lightning library to build the model pipeline from the based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2acee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.lightning import CHBMITPreprocessedDataModule\n",
    "\n",
    "datamodule = CHBMITPreprocessedDataModule(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e24755af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from encoder import TBREncoder\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from model.classifier import ModelConfig\n",
    "from model.lightning import OptimizerConfig, LitSTFTPreprocessedSeizureClassifier\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "\n",
    "def objective_tbr(trial: optuna.trial.Trial) -> float:\n",
    "    model_params: ModelConfig = {\n",
    "        \"threshold\": trial.suggest_float(\"threshold\", 0.01, 0.5),\n",
    "        \"slope\": trial.suggest_float(\"slope\", 1.0, 20.0),\n",
    "        \"beta\": trial.suggest_float(\"beta\", 0.1, 0.99),\n",
    "        \"dropout_rate1\": trial.suggest_float(\"dropout_rate1\", 0.1, 0.99),\n",
    "        \"dropout_rate2\": trial.suggest_float(\"dropout_rate2\", 0.1, 0.99),\n",
    "    }\n",
    "\n",
    "    optimizer_params: OptimizerConfig = {\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-6, 1e-4, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-4, log=True),\n",
    "        \"scheduler_factor\": trial.suggest_float(\"scheduler_factor\", 0.1, 0.99),\n",
    "        \"scheduler_patience\": trial.suggest_int(\"scheduler_patience\", 1, 10),\n",
    "    }\n",
    "    encoder_threshold = trial.suggest_float(\"encoder_threshold\", 0.1, 0.99, log=True)\n",
    "    \n",
    "    spike_encoder = TBREncoder(threshold=encoder_threshold, normalize=False)\n",
    "\n",
    "    model = LitSTFTPreprocessedSeizureClassifier(\n",
    "        model_config=model_params,\n",
    "        optimizer_config=optimizer_params,\n",
    "        spike_encoder=spike_encoder,\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=15,\n",
    "        accelerator=\"auto\",\n",
    "        devices=\"auto\",\n",
    "        strategy=\"auto\",\n",
    "        callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")],\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36d5cf32-5a13-441e-bc01-98b11e1c488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from encoder import PoissonEncoder\n",
    "# from optuna.integration import PyTorchLightningPruningCallback\n",
    "# from model.classifier import ModelConfig\n",
    "# from model.lightning import OptimizerConfig, LitSTFTPreprocessedSeizureClassifier\n",
    "# import lightning.pytorch as pl\n",
    "\n",
    "\n",
    "# def objective_rate(trial: optuna.trial.Trial) -> float:\n",
    "#     model_params: ModelConfig = {\n",
    "#         \"threshold\": trial.suggest_float(\"threshold\", 0.01, 0.5),\n",
    "#         \"slope\": trial.suggest_float(\"slope\", 1.0, 20.0),\n",
    "#         \"beta\": trial.suggest_float(\"beta\", 0.1, 0.99),\n",
    "#         \"dropout_rate1\": trial.suggest_float(\"dropout_rate1\", 0.1, 0.99),\n",
    "#         \"dropout_rate2\": trial.suggest_float(\"dropout_rate2\", 0.1, 0.99),\n",
    "#     }\n",
    "\n",
    "#     optimizer_params: OptimizerConfig = {\n",
    "#         \"lr\": trial.suggest_float(\"lr\", 1e-6, 1e-4, log=True),\n",
    "#         \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-4, log=True),\n",
    "#         \"scheduler_factor\": trial.suggest_float(\"scheduler_factor\", 0.1, 0.99),\n",
    "#         \"scheduler_patience\": trial.suggest_int(\"scheduler_patience\", 1, 10),\n",
    "#     }\n",
    "#     params = {\n",
    "#         \"interval_freq\": trial.suggest_int(\"interval_freq\", 1, 16),\n",
    "#         \"normalize\": False\n",
    "#     }\n",
    "\n",
    "#     spike_encoder = PoissonEncoder(**params)\n",
    "\n",
    "#     model = LitSTFTPreprocessedSeizureClassifier(\n",
    "#         model_config=model_params,\n",
    "#         optimizer_config=optimizer_params,\n",
    "#         spike_encoder=spike_encoder,\n",
    "#     )\n",
    "\n",
    "#     trainer = pl.Trainer(\n",
    "#         max_epochs=15,\n",
    "#         accelerator=\"auto\",\n",
    "#         devices=\"auto\",\n",
    "#         strategy=\"auto\",\n",
    "#         callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")],\n",
    "#     )\n",
    "\n",
    "#     trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "#     return trainer.callback_metrics[\"val_loss\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f70c42ae-5234-4f35-87ba-f228d245acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from encoder import BurstEncoder\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from model.classifier import ModelConfig\n",
    "from model.lightning import OptimizerConfig, LitSTFTPreprocessedSeizureClassifier\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "\n",
    "def objective_be(trial: optuna.trial.Trial) -> float:\n",
    "    model_params: ModelConfig = {\n",
    "        \"threshold\": trial.suggest_float(\"threshold\", 0.01, 0.5),\n",
    "        \"slope\": trial.suggest_float(\"slope\", 1.0, 20.0),\n",
    "        \"beta\": trial.suggest_float(\"beta\", 0.1, 0.99),\n",
    "        \"dropout_rate1\": trial.suggest_float(\"dropout_rate1\", 0.1, 0.99),\n",
    "        \"dropout_rate2\": trial.suggest_float(\"dropout_rate2\", 0.1, 0.99),\n",
    "    }\n",
    "\n",
    "    optimizer_params: OptimizerConfig = {\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-6, 1e-4, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-4, log=True),\n",
    "        \"scheduler_factor\": trial.suggest_float(\"scheduler_factor\", 0.1, 0.99),\n",
    "        \"scheduler_patience\": trial.suggest_int(\"scheduler_patience\", 1, 10),\n",
    "    }\n",
    "    t_min = trial.suggest_int(\"t_min\", 1, 16)\n",
    "    t_max = trial.suggest_int(\"t_max\", t_min, 16)\n",
    "\n",
    "    params = {\n",
    "        \"max_window\": trial.suggest_int(\"max_window\", 1, 16),\n",
    "        \"n_max\": trial.suggest_int(\"n_max\", 1, 16),\n",
    "        \"t_min\": t_min,\n",
    "        \"t_max\": t_max,\n",
    "        \"normalize\": False\n",
    "    }\n",
    "\n",
    "\n",
    "    spike_encoder = BurstEncoder(**params)\n",
    "\n",
    "    model = LitSTFTPreprocessedSeizureClassifier(\n",
    "        model_config=model_params,\n",
    "        optimizer_config=optimizer_params,\n",
    "        spike_encoder=spike_encoder,\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=15,\n",
    "        accelerator=\"auto\",\n",
    "        devices=\"auto\",\n",
    "        strategy=\"auto\",\n",
    "        callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")],\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83b94359-a16b-467f-8c92-5432b08ebac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from encoder import DummyEncoder\n",
    "# from optuna.integration import PyTorchLightningPruningCallback\n",
    "# from model.classifier import ModelConfig\n",
    "# from model.lightning import OptimizerConfig, LitSTFTPreprocessedSeizureClassifier\n",
    "# import lightning.pytorch as pl\n",
    "\n",
    "\n",
    "# def objective_base(trial: optuna.trial.Trial) -> float:\n",
    "#     model_params: ModelConfig = {\n",
    "#         \"threshold\": trial.suggest_float(\"threshold\", 0.01, 0.5),\n",
    "#         \"slope\": trial.suggest_float(\"slope\", 1.0, 20.0),\n",
    "#         \"beta\": trial.suggest_float(\"beta\", 0.1, 0.99),\n",
    "#         \"dropout_rate1\": trial.suggest_float(\"dropout_rate1\", 0.1, 0.99),\n",
    "#         \"dropout_rate2\": trial.suggest_float(\"dropout_rate2\", 0.1, 0.99),\n",
    "#     }\n",
    "\n",
    "#     optimizer_params: OptimizerConfig = {\n",
    "#         \"lr\": trial.suggest_float(\"lr\", 1e-6, 1e-4, log=True),\n",
    "#         \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-4, log=True),\n",
    "#         \"scheduler_factor\": trial.suggest_float(\"scheduler_factor\", 0.1, 0.99),\n",
    "#         \"scheduler_patience\": trial.suggest_int(\"scheduler_patience\", 1, 10),\n",
    "#     }\n",
    "\n",
    "#     spike_encoder = DummyEncoder()\n",
    "\n",
    "#     model = LitSTFTPreprocessedSeizureClassifier(\n",
    "#         model_config=model_params,\n",
    "#         optimizer_config=optimizer_params,\n",
    "#         spike_encoder=spike_encoder,\n",
    "#     )\n",
    "\n",
    "#     trainer = pl.Trainer(\n",
    "#         max_epochs=15,\n",
    "#         accelerator=\"auto\",\n",
    "#         devices=\"auto\",\n",
    "#         strategy=\"auto\",\n",
    "#         callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")],\n",
    "#     )\n",
    "\n",
    "#     trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "#     return trainer.callback_metrics[\"val_loss\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53dd58c1-4018-4074-828e-ffa531a08b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import DB_CONN_STRING\n",
    "import optuna\n",
    "\n",
    "def create_and_run_study(study_name: str, objective, n_trials: int):\n",
    "\n",
    "    sampler = optuna.samplers.CmaEsSampler()\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        study_name=study_name,\n",
    "        sampler=sampler,\n",
    "        storage=DB_CONN_STRING,\n",
    "        load_if_exists=True,\n",
    "        pruner=optuna.pruners.HyperbandPruner()\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "695a3beb-5ca2-443f-917e-ca334634fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = [\n",
    "    # (\"Classifier_TBR_Tuning\", objective_tbr, 100),\n",
    "    (\"Classifier_BE_Tuning\", objective_be, 100)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaa5dbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 14:21:09,327] A new study created in RDB with name: Classifier_BE_Tuning\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type               | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model | EEGSpikeClassifier | 824 K  | train\n",
      "-----------------------------------------------------\n",
      "824 K     Trainable params\n",
      "0         Non-trainable params\n",
      "824 K     Total params\n",
      "3.299     Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc9ea6e29784847ba33fdbee8ea8e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-05-02 14:21:10,282] Trial 0 failed with parameters: {'threshold': 0.43167924203892594, 'slope': 3.3384099767505147, 'beta': 0.9742631434388955, 'dropout_rate1': 0.653976385990211, 'dropout_rate2': 0.6874588543612099, 'lr': 3.4801008169401985e-06, 'weight_decay': 2.462070744524151e-05, 'scheduler_factor': 0.166906470563762, 'scheduler_patience': 2, 't_min': 5, 't_max': 11, 'max_window': 11, 'n_max': 11} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 2.38 MiB is free. Process 3098062 has 3.52 GiB memory in use. Process 3427738 has 5.06 GiB memory in use. Process 3383238 has 11.74 GiB memory in use. Process 3427694 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 492.00 MiB memory in use. Of the allocated memory 25.67 MiB is allocated by PyTorch, and 20.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-group55/.local/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3880785/1199026012.py\", line 52, in objective_be\n",
      "    trainer.fit(model, datamodule=datamodule)\n",
      "  File \"/home/jupyter-group55/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/jupyter-group55/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jupyter-group55/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/jupyter-group55/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jupyter-group55/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/jupyter-group55/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/jupyter-group55/.local/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jupyter-group55/.local/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/jupyter-group55/.local/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jupyter-group55/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jupyter-group55/.local/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jupyter-group55/snn-encoder-test/model/lightning.py\", line 178, in validation_step\n",
      "    data_spike = self.spike_encoder.encode(data)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jupyter-group55/snn-encoder-test/encoder/latency.py\", line 88, in encode\n",
      "    padded_x = torch.nn.functional.pad(x, (0, padding), value=float(\"nan\"))\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jupyter-group55/.local/lib/python3.12/site-packages/torch/nn/functional.py\", line 5209, in pad\n",
      "    return torch._C._nn.pad(input, pad, mode, value)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 2.38 MiB is free. Process 3098062 has 3.52 GiB memory in use. Process 3427738 has 5.06 GiB memory in use. Process 3383238 has 11.74 GiB memory in use. Process 3427694 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 492.00 MiB memory in use. Of the allocated memory 25.67 MiB is allocated by PyTorch, and 20.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[W 2025-05-02 14:21:10,284] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 2.38 MiB is free. Process 3098062 has 3.52 GiB memory in use. Process 3427738 has 5.06 GiB memory in use. Process 3383238 has 11.74 GiB memory in use. Process 3427694 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 492.00 MiB memory in use. Of the allocated memory 25.67 MiB is allocated by PyTorch, and 20.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, objective_fn, n_trials \u001b[38;5;129;01min\u001b[39;00m experiment:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mcreate_and_run_study\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mcreate_and_run_study\u001b[39m\u001b[34m(study_name, objective, n_trials)\u001b[39m\n\u001b[32m      6\u001b[39m sampler = optuna.samplers.CmaEsSampler()\n\u001b[32m      7\u001b[39m study = optuna.create_study(\n\u001b[32m      8\u001b[39m     direction=\u001b[33m\"\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     study_name=study_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     pruner=optuna.pruners.HyperbandPruner()\n\u001b[32m     14\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m study\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/optuna/study/study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/optuna/study/_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/optuna/study/_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mobjective_be\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     38\u001b[39m model = LitSTFTPreprocessedSeizureClassifier(\n\u001b[32m     39\u001b[39m     model_config=model_params,\n\u001b[32m     40\u001b[39m     optimizer_config=optimizer_params,\n\u001b[32m     41\u001b[39m     spike_encoder=spike_encoder,\n\u001b[32m     42\u001b[39m )\n\u001b[32m     44\u001b[39m trainer = pl.Trainer(\n\u001b[32m     45\u001b[39m     max_epochs=\u001b[32m15\u001b[39m,\n\u001b[32m     46\u001b[39m     accelerator=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     callbacks=[PyTorchLightningPruningCallback(trial, monitor=\u001b[33m\"\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m\"\u001b[39m)],\n\u001b[32m     50\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trainer.callback_metrics[\u001b[33m\"\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m\"\u001b[39m].item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1017\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1054\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1053\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m   1056\u001b[39m         \u001b[38;5;28mself\u001b[39m.fit_loop.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1083\u001b[39m, in \u001b[36mTrainer._run_sanity_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1080\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1082\u001b[39m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m \u001b[43mval_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1085\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1087\u001b[39m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:179\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py:145\u001b[39m, in \u001b[36m_EvaluationLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_progress.is_last_batch = data_fetcher.done\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    147\u001b[39m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py:437\u001b[39m, in \u001b[36m_EvaluationLoop._evaluation_step\u001b[39m\u001b[34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[39m\n\u001b[32m    431\u001b[39m hook_name = \u001b[33m\"\u001b[39m\u001b[33mtest_step\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.testing \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    432\u001b[39m step_args = (\n\u001b[32m    433\u001b[39m     \u001b[38;5;28mself\u001b[39m._build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[32m    436\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_progress.increment_processed()\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[32m    442\u001b[39m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    331\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py:412\u001b[39m, in \u001b[36mStrategy.validation_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/snn-encoder-test/model/lightning.py:178\u001b[39m, in \u001b[36mLitSTFTPreprocessedSeizureClassifier.validation_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[32m    177\u001b[39m     data, targets = batch\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     data_spike = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspike_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;66;03m# Predict the spikes using the model\u001b[39;00m\n\u001b[32m    181\u001b[39m     spk_rec, _ = \u001b[38;5;28mself\u001b[39m.model(data_spike)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/snn-encoder-test/encoder/latency.py:88\u001b[39m, in \u001b[36mBurstEncoder.encode\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     84\u001b[39m batch, channels, freqs, time_steps = x.shape\n\u001b[32m     86\u001b[39m padding = -time_steps % \u001b[38;5;28mself\u001b[39m.max_window\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m padded_x = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m downsampled_x = padded_x.reshape(batch * channels * freqs, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.max_window).nanmean(dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     92\u001b[39m spike_index = torch.arange(\u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m.max_window, \u001b[32m1\u001b[39m).expand((batch * channels * freqs, downsampled_x.shape[\u001b[32m1\u001b[39m], \u001b[38;5;28mself\u001b[39m.max_window))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/functional.py:5209\u001b[39m, in \u001b[36mpad\u001b[39m\u001b[34m(input, pad, mode, value)\u001b[39m\n\u001b[32m   5202\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mreplicate\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   5203\u001b[39m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put.\u001b[39;00m\n\u001b[32m   5204\u001b[39m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[32m   5205\u001b[39m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[32m   5206\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\n\u001b[32m   5207\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtorch._decomp.decompositions\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5208\u001b[39m             )._replication_pad(\u001b[38;5;28minput\u001b[39m, pad)\n\u001b[32m-> \u001b[39m\u001b[32m5209\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 2.38 MiB is free. Process 3098062 has 3.52 GiB memory in use. Process 3427738 has 5.06 GiB memory in use. Process 3383238 has 11.74 GiB memory in use. Process 3427694 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 492.00 MiB memory in use. Of the allocated memory 25.67 MiB is allocated by PyTorch, and 20.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "for name, objective_fn, n_trials in experiment:\n",
    "    create_and_run_study(name, objective_fn, n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ffea26-85b2-43a5-8197-9a476e52e9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
